{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read large data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readin_large_data():\n",
    "    \"\"\"Read large data set into a 2D numpy array\n",
    "    return: numpy array contains the small data set, shape: (50,000, 15000)\n",
    "    \"\"\"   \n",
    "    PATH = os.getcwd()\n",
    "    \n",
    "    data_path = os.path.join(PATH, 'data_large', 'orange_large_train.data','orange_large_train.data.chunk1')\n",
    "    \n",
    "    with open(data_path) as f:\n",
    "        header = f.readline().strip('\\n').split('\\t')\n",
    "    \n",
    "    data_type = {key:np.float64 for key in header[:14740]}\n",
    "    data_type.update({key:str for key in header[14740:]})\n",
    "    X = pd.read_table(data_path, dtype=data_type)\n",
    "\n",
    "    for i in range(2, 6):\n",
    "        data_path = os.path.join(PATH,'data_large', 'orange_large_train.data','orange_large_train.data.chunk' + str(i))\n",
    "        \n",
    "        temp = pd.read_table(data_path, header=None, dtype=data_type)\n",
    "        temp.columns=header\n",
    "        X = X.append(temp)\n",
    "    \n",
    "    labels = []\n",
    "    for target in ['upselling', 'churn', 'appetency']:\n",
    "        PATH = os.getcwd()\n",
    "        \n",
    "        LABEL_PATH = os.path.join(PATH, 'data_large','orange_large_train_'+target+'.labels')\n",
    "\n",
    "        label = pd.read_csv(LABEL_PATH, header=None, delimiter='\\t')\n",
    "        label[label==1] = True\n",
    "        label[label==-1] = False\n",
    "        labels.append(label)\n",
    "    \n",
    "    y = np.hstack(labels)\n",
    "    y = np.hstack([y, ~np.any(y, axis=1)[:,np.newaxis]])\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1717961/miniconda2/envs/mlp/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2802: DtypeWarning: Columns (14741,14742,14748,14749,14750,14752,14753,14755,14758,14759,14760,14762,14767,14769,14773,14776,14779,14780,14781,14789,14791,14797,14798,14800,14804,14805,14806,14809,14811,14817,14818,14819,14824,14828,14832,14834,14836,14843,14844,14850,14851,14855,14861,14871,14876,14878,14879,14881,14882,14883,14886,14888,14890,14894,14896,14897,14900,14904,14906,14914,14916,14917,14918,14926,14928,14931,14932,14936,14937,14939,14942,14944,14947,14951,14954,14957,14958,14970,14972,14980,14996,14998) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "X, y = readin_large_data()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to training set, validation set and test set by 8:1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.1, random_state=666)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=1/9, random_state=888)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the variables where the amounts of the missing data are large, and fill in the missing data in remaining variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For numerical data\n",
    "numerical_train = X_train.iloc[:, :-260]\n",
    "numerical_val = X_val.iloc[:, :-260]\n",
    "numerical_test = X_test.iloc[:, :-260]\n",
    "\n",
    "numerical_missing_rate = numerical_train.isnull().sum() / numerical_train.shape[0]\n",
    "useful_numerical_train = numerical_train.loc[:, numerical_missing_rate < 0.5]\n",
    "useful_numerical_val = numerical_val.loc[:, numerical_missing_rate < 0.5]\n",
    "useful_numerical_test = numerical_test.loc[:, numerical_missing_rate < 0.5]\n",
    "\n",
    "clean_numerical_train = useful_numerical_train.fillna(useful_numerical_train.mean())\n",
    "clean_numerical_val = useful_numerical_val.fillna(useful_numerical_train.mean())\n",
    "clean_numerical_test = useful_numerical_test.fillna(useful_numerical_train.mean())\n",
    "\n",
    "# For categorical data\n",
    "categorical_train = X_train.iloc[:, -260:]\n",
    "categorical_val = X_val.iloc[:, -260:]\n",
    "categorical_test = X_test.iloc[:, -260:]\n",
    "\n",
    "categorical_missing_rate = categorical_train.isnull().sum() / categorical_train.shape[0]\n",
    "useful_categorical_train = categorical_train.loc[:, categorical_missing_rate < 0.5]\n",
    "useful_categorical_val = categorical_val.loc[:, categorical_missing_rate < 0.5]\n",
    "useful_categorical_test = categorical_test.loc[:, categorical_missing_rate < 0.5]\n",
    "\n",
    "clean_categorical_train = useful_categorical_train.fillna(useful_categorical_train.mode().iloc[0])\n",
    "clean_categorical_val = useful_categorical_val.fillna(useful_categorical_train.mode().iloc[0])\n",
    "clean_categorical_test = useful_categorical_test.fillna(useful_categorical_train.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 14438)\n",
      "(5000, 14438)\n",
      "(5000, 14438)\n",
      "(40000, 14438)\n",
      "(5000, 14438)\n",
      "(5000, 14438)\n",
      "(40000, 36)\n",
      "(5000, 36)\n",
      "(5000, 36)\n",
      "(40000, 36)\n",
      "(5000, 36)\n",
      "(5000, 36)\n"
     ]
    }
   ],
   "source": [
    "print(useful_numerical_train.shape)\n",
    "print(useful_numerical_val.shape)\n",
    "print(useful_numerical_test.shape)\n",
    "print(clean_numerical_train.shape)\n",
    "print(clean_numerical_val.shape)\n",
    "print(clean_numerical_test.shape)\n",
    "print(useful_categorical_train.shape)\n",
    "print(useful_categorical_val.shape)\n",
    "print(useful_categorical_test.shape)\n",
    "print(clean_categorical_train.shape)\n",
    "print(clean_categorical_val.shape)\n",
    "print(clean_categorical_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do one-hot to categorical data, only remaining the variables whose category number is in [2, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def do_one_hot(categorical_data, train_len, threshold=100):\n",
    "    useful_index = []\n",
    "    for i in range(categorical_data.shape[1]):\n",
    "        variances = categorical_data.iloc[:,i]\n",
    "        uni_len = len(variances[:train_len].unique())\n",
    "        if uni_len>=2 and uni_len<=threshold:\n",
    "            useful_index.append(i)\n",
    "    return pd.get_dummies(categorical_data.iloc[:,useful_index])\n",
    "    \n",
    "one_hot_data = do_one_hot(pd.concat([clean_categorical_train, clean_categorical_val, clean_categorical_val], ignore_index=True), clean_categorical_train.shape[0])\n",
    "\n",
    "one_hot_train = one_hot_data.iloc[:clean_categorical_train.shape[0],:]\n",
    "one_hot_val = one_hot_data.iloc[clean_categorical_train.shape[0]:clean_categorical_train.shape[0]+clean_categorical_val.shape[0],:]\n",
    "one_hot_test = one_hot_data.iloc[-clean_categorical_test.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 442)\n",
      "(5000, 442)\n",
      "(5000, 442)\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_train.shape)\n",
    "print(one_hot_val.shape)\n",
    "print(one_hot_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the numerical data and categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_numerical_train.index = one_hot_train.index\n",
    "clean_numerical_val.index = one_hot_val.index\n",
    "clean_numerical_test.index = one_hot_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_preprocessed = pd.concat([clean_numerical_train,one_hot_train], axis=1, ignore_index=True)\n",
    "X_val_preprocessed = pd.concat([clean_numerical_val,one_hot_val], axis=1, ignore_index=True)\n",
    "X_test_preprocessed = pd.concat([clean_numerical_test,one_hot_test], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 14880)\n",
      "(5000, 14880)\n",
      "(5000, 14880)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_preprocessed.shape)\n",
    "print(X_val_preprocessed.shape)\n",
    "print(X_test_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "mean_train = X_train_preprocessed.mean(axis=0)\n",
    "std_train = X_train_preprocessed.std(axis=0)\n",
    "\n",
    "X_train_scaled = (X_train_preprocessed - mean_train)/(std_train+1e-8)\n",
    "X_val_scaled = (X_val_preprocessed - mean_train)/(std_train+1e-8)\n",
    "X_test_scaled = (X_test_preprocessed - mean_train)/(std_train+1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_val.npy', y_val)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensions. (Choose one of the following methods.）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "LOW_DIMENSION = 1000\n",
    "\n",
    "pca = PCA(n_components=LOW_DIMENSION).fit(X_train_scaled)\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 1000)\n",
      "(5000, 1000)\n",
      "(5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pca.shape)\n",
    "print(X_val_pca.shape)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('X_train_pca_'+str(LOW_DIMENSION), X_train_pca)\n",
    "np.save('X_val_pca_'+str(LOW_DIMENSION), X_val_pca)\n",
    "np.save('X_test_pca_'+str(LOW_DIMENSION), X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isomap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "# try some configurations of the following hyperparameters.\n",
    "LOW_DIMENSION = 2\n",
    "NUM_NEIGHBORS = 3\n",
    "\n",
    "isomap = Isomap(n_neighbors=NUM_NEIGHBORS, n_components=LOW_DIMENSION).fit(X_train_scaled)\n",
    "X_train_iso = isomap.transform(X_train_scaled)\n",
    "X_val_iso = isomap.transform(X_val_scaled)\n",
    "X_test_iso = isomap.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train_iso.shape)\n",
    "print(X_val_iso.shape)\n",
    "print(X_test_iso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('X_train_iso_'+str(LOW_DIMENSION)+'_'+str(NUM_NEIGHBORS), X_train_iso)\n",
    "np.save('X_val_iso_'+str(LOW_DIMENSION)+'_'+str(NUM_NEIGHBORS), X_val_iso)\n",
    "np.save('X_test_iso_'+str(LOW_DIMENSION)+'_'+str(NUM_NEIGHBORS), X_test_iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocalLinearEmbedding\n",
    "\n",
    "# try some configurations of the following hyperparameters.\n",
    "LOW_DIMENSION = 2\n",
    "NUM_NEIGHBORS = 3\n",
    "\n",
    "lle = LocalLinearEmbedding(n_neighbors=NUM_NEIGHBORS, n_components=LOW_DIMENSION).fit(X_train_scaled)\n",
    "X_train_lle = lle.transform(X_train_scaled)\n",
    "X_val_lle = lle.transform(X_val_scaled)\n",
    "X_test_lle = lle.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train_iso.shape)\n",
    "print(X_val_iso.shape)\n",
    "print(X_test_iso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('X_train_lle_'+str(LOW_DIMENSION)+'_'+str(NUM_NEIGHBORS), X_train_lle)\n",
    "np.save('X_val_lle_'+str(LOW_DIMENSION)+'_'+str(NUM_NEIGHBORS), X_val_lle)\n",
    "np.save('X_test_lle_'+str(LOW_DIMENSION)+'_'+str(NUM_NEIGHBORS), X_test_lle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
