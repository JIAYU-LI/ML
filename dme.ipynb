{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy.random as rng\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Lambda\n",
    "from keras.layers import Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "import os\n",
    "from os.path import join, dirname, isfile, abspath\n",
    "from keras.preprocessing.image import img_to_array, load_img #convert image to array\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(1337)  # for the sake of reproducibility\n",
    "\n",
    "def W_INIT(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "#CNN_MODEL_FILE = join(MODELS_DIR, 'cnn.h5')\n",
    "nb_epoch = 10\n",
    "BATCH_SIZE = 48\n",
    "L2_REG = 0.03\n",
    "#W_INIT = 'he_normal'\n",
    "LAST_FEATURE_MAPS_LAYER = 46\n",
    "LAST_FEATURE_MAPS_SIZE = (128, 8, 8)\n",
    "PENULTIMATE_LAYER = 51\n",
    "PENULTIMATE_SIZE = 2048\n",
    "SOFTMAX_LAYER = 55\n",
    "SOFTMAX_SIZE = 475\n",
    "\n",
    "def readin_small_data():\n",
    "    \"\"\"Read small data set into a 2D numpy array\n",
    "    return: numpy array contains the small data set, shape: (50,000, 230)\n",
    "    \"\"\"\n",
    "    PATH = os.getcwd()\n",
    "    DATA = os.path.join(PATH, 'orange_small_train','orange_small_train.data')\n",
    "    with open(DATA) as f:\n",
    "        header = f.readline().strip('\\n').split('\\t')\n",
    "    \n",
    "    data_type = {key:np.float64 for key in header[:190]}\n",
    "    data_type.update({key:str for key in header[190:]})\n",
    "    return pd.read_table(DATA, dtype=data_type)\n",
    "\n",
    "def readin_label(target):\n",
    "    \"\"\"Read upselling, churn or appetency label into a 2D numpy array\n",
    "    parameter target: choose which label to load\n",
    "    return: numpy array contains the specified label of small data set, shape: (50,000, 1)\n",
    "    \"\"\"\n",
    "    assert target in ['upselling', 'churn', 'appetency']\n",
    "    PATH = os.getcwd()\n",
    "    DATA = os.path.join(PATH, 'orange_small_train','orange_small_train_'+target+'.labels.txt')\n",
    "\n",
    "    with open(DATA) as f:\n",
    "        lines = f.readlines()\n",
    "        data = list(map(lambda x: True if x.strip() == '1' else False, lines))\n",
    "\n",
    "    return np.array(data)[:,np.newaxis]\n",
    "\n",
    "model.add(BatchNormalization(axis=1, mode=0))\n",
    "    model.add(PReLU(init=W_INIT))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(p=0.5))\n",
    "    model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dnn(data_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(32, input_shape=(data_dim)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cnn_network(imgs_dim, compile_=True):\n",
    "    \"\"\" Base network to be shared (eq. to feature extraction).\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(_convolutional_layer(nb_filter=16, input_shape=imgs_dim))\n",
    "    model.add(BatchNormalization(axis=1, mode=0))\n",
    "    model.add(PReLU(init=W_INIT))\n",
    "    model.add(_convolutional_layer(nb_filter=16))\n",
    "    model.add(BatchNormalization(axis=1, mode=0))\n",
    "    model.add(PReLU(init=W_INIT))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th'))\n",
    "\n",
    "    model.add(_convolutional_layer(nb_filter=32))\n",
    "    model.add(BatchNormalization(axis=1, mode=0))\n",
    "    model.add(PReLU(init=W_INIT))\n",
    "    model.add(_convolutional_layer(nb_filter=32))\n",
    "    model.add(BatchNormalization(axis=1, mode=0))\n",
    "    model.add(PReLU(init=W_INIT))\n",
    "    model.add(_convolutional_layer(nb_filter=32))\n",
    "    model.add(BatchNormalization(axis=1, mode=0))\n",
    "    model.add(PReLU(init=W_INIT))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th'))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Dropout(p=0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(_dense_layer(output_dim=PENULTIMATE_SIZE))\n",
    "    model.add(BatchNormalization(mode=0))\n",
    "    model.add(PReLU(init=W_INIT))\n",
    "\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "def _convolutional_layer(nb_filter, input_shape=None):\n",
    "    if input_shape:\n",
    "        return _first_convolutional_layer(nb_filter, input_shape)\n",
    "    else:\n",
    "        return _intermediate_convolutional_layer(nb_filter)\n",
    "\n",
    "\n",
    "def _first_convolutional_layer(nb_filter, input_shape):\n",
    "    return Conv2D(\n",
    "        nb_filter=nb_filter, nb_row=3, nb_col=3, input_shape=input_shape,\n",
    "        border_mode='same', init=W_INIT, W_regularizer=l2(l=L2_REG))\n",
    "\n",
    "\n",
    "def _intermediate_convolutional_layer(nb_filter):\n",
    "    return Conv2D(\n",
    "        nb_filter=nb_filter, nb_row=3, nb_col=3, border_mode='same',\n",
    "        init=W_INIT, W_regularizer=l2(l=L2_REG))\n",
    "\n",
    "\n",
    "def _dense_layer(output_dim):\n",
    "    return Dense(output_dim=output_dim, W_regularizer=l2(l=L2_REG), init=W_INIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'mj86'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f564c726d28b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappetency_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/mlp/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda2/envs/mlp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda2/envs/mlp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/mlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/miniconda2/envs/mlp/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'mj86'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = readin_small_data()\n",
    "upselling_label = readin_label('upselling')\n",
    "appetency_label = readin_label('appetency')\n",
    "\n",
    "appetency_label = appetency_label.astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=100, activation='relu', input_dim=230))\n",
    "#model.add(Dense(units=100, activation='relu'))\n",
    "#model.add(Dense(units=100, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=1, activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=adam,\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "trainX, valX, trainY, valY = train_test_split(data, appetency_label, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data = (valX, valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
