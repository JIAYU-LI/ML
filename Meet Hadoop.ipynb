{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now test Hadoop by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a listing of directories on HDFS, you've successfully configured everything. If not, make sure you do ALL of the described steps EXACTLY as they appear in this document. Note that you should not continue if you have not managed to do this section. If the hdfs command isn't available, contact a demonstrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a number of small pointers you should work through to familiarise yourself with navigating around HDFS\n",
    "\n",
    "    Make sure that your home directory exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/$USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a directory called /user/$USER/data in Hadoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir /user/$USER/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following directories in a similar way (these directories will NOT have been created for you, so you need to create them yourself):\n",
    "\n",
    "    /user/$USER/data/input\n",
    "    /user/$USER/data/output\n",
    "    /user/$USER/source\n",
    "Confirm that you've done the right thing by typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/$USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if your matriculation number is s0123456, you should see something like:\n",
    "    Found 2 items\n",
    "drwxr-xr-x   - s0123456 s0123456          0 2011-10-19 09:55 /user/s0123456/data\n",
    "drwxr-xr-x   - s0123456 s0123456          0 2011-10-19 09:54 /user/s0123456/source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the file example1.txt to /user/$USER/data/output by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -cp /data/labs/example1.txt /user/$USER/data/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might warn you about DFSInputStream. Just ignore that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, example1.txt doesn't belong there. Move it from /user/$USER/data/output to /user/$USER/data/input where it belongs and delete the /user/$USER/data/output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -mv /user/$USER/data/output/example1.txt /user/$USER/data/input/\n",
    "hdfs dfs -rm -r /user/$USER/data/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the contents of example1.txt using cat and then tail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/$USER/data/input/example1.txt\n",
    "hdfs dfs -tail /user/$USER/data/input/example1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty file named example2 in /user/$USER/data/input. Use test to check if it exists and that it is indeed zero length; (by validating the environment variable $? is equal to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -touchz /user/$USER/data/input/example2\n",
    "hdfs dfs -test -z /user/$USER/data/input/example2; echo $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the file example2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -rm /user/$USER/data/input/example2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of HDFS Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is a list of useful HDFS shell commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)cat -- copy files to stdout, similar to UNIX cat command:\n",
    "\n",
    "hdfs dfs -cat /user/$USER/data/input/example1.txt\n",
    "\n",
    "2)copyFromLocal -- copy single source, or multiple sources from local file system to the destination filesystem. Source must be a local file reference:\n",
    "\n",
    "hdfs dfs -copyFromLocal <localfile> /user/$USER/file1\n",
    "\n",
    "3)copyToLocal -- copy files to the local file system. Destination must be a local file reference.\n",
    "\n",
    "hdfs dfs -copyToLocal /user/$USER/file1 <localfile>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:\n",
    "\n",
    "    -ignoreCrc -- files that fail the CRC check will be copied.\n",
    "    -crc -- files and CRCs will be copied.\n",
    "\n",
    "cp -- copy files from source to destination. This command allows multiple sources as well in which case the destination must be a directory. Similar to UNIX cp command.\n",
    "\n",
    "hdfs dfs -cp /user/$USER/file1 /user/$USER/file2\n",
    "\n",
    "getmerge -- take a source directory and a destination file as input and concatenate files in src into the destination local file. Optionally -nl can be set to enable adding a newline character at the end of each file.\n",
    "\n",
    "hdfs dfs -getmerge /data/labs/example-data ~/result_file\n",
    "\n",
    "ls -- for a file returns stat on the file with the format: filename num_replicas size modification_date modification_time permissions userid groupid\n",
    "\n",
    "For a directory it returns list of its direct children as in UNIX, with the format: dirname <dir> modification_time modification_time permissions userid groupid\n",
    "\n",
    "hdfs dfs -ls /user/$USER\n",
    "\n",
    "You can also pass -R for recursive listing.\n",
    "\n",
    "mkdir -- create a directory.\n",
    "\n",
    "hdfs dfs -mkdir /user/$USER/deleteme\n",
    "\n",
    "You can pass -p to make directories along a path\n",
    "\n",
    "hdfs dfs -mkdir -p /user/$USER/deleteme/and/this\n",
    "\n",
    "mv -- move files from source to destination similar to UNIX mv command. This command allows multiple sources as well in which case the destination needs to be a directory. Moving files across filesystems is not permitted.\n",
    "\n",
    "hdfs dfs -mv /user/$USER/file1 /user/$USER/file2\n",
    "\n",
    "rm -- delete files, similar to UNIX rm command. Only deletes empty directories and files.\n",
    "\n",
    "hdfs dfs -rm /user/$USER/file1\n",
    "\n",
    "Also supports -r to recursively delete files like rm -r on UNIX.\n",
    "\n",
    "tail -- Displays last kilobyte of the file to stdout. Similar to UNIX tail command.\n",
    "\n",
    "hdfs dfs -tail /user/$USER/file1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:\n",
    "\n",
    "    -f output appended data as the file grows (follow)\n",
    "\n",
    "test -- perform various test.\n",
    "\n",
    "hdfs dfs -test -e /user/$USER/file1\n",
    "\n",
    "Options:\n",
    "\n",
    "    -e check to see if the file exists. Return 0 if true.\n",
    "    -z check to see if the file is zero length. Return 0 if true.\n",
    "    -d check return 1 if the path is directory else return 0.\n",
    "\n",
    "-test returns the value of its test (0 or 1) to the environment variable $?, to view its value enter the following into your terminal:\n",
    "\n",
    "echo $?\n",
    "\n",
    "touchz -- create a file of zero length. Similar to UNIX touch command.\n",
    "\n",
    "hdfs dfs -touchz /user/$USER/file1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Managing Hadoop Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Command Line\n",
    "Listing Jobs\n",
    "\n",
    "Running the following command will list all (completed and running) jobs on the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapred job -list all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This will result in a table like this being displayed:\n",
    "JobId                 State     StartTime     UserName Priority SchedulingInfo\n",
    "job_201009151640_0001 2         1285081662441 s0894589 NORMAL   NA\n",
    "job_201009151640_0002 2         1285081976156 s0894589 NORMAL   NA\n",
    "job_201009151640_0003 2         1285082017461 s0894589 NORMAL   NA\n",
    "job_201009151640_0004 2         1285082159071 s0894589 NORMAL   NA\n",
    "job_201009151640_0005 2         1285082189917 s0894589 NORMAL   NA\n",
    "job_201009151640_0006 2         1285082275965 s0894589 NORMAL   NA\n",
    "job_201009151640_0009 2         1285083343068 s0894589 NORMAL   NA\n",
    "job_201009151640_0010 3         1285106676902 s0894589 NORMAL   NA\n",
    "job_201009151640_0012 3         1285106959588 s0894589 NORMAL   NA\n",
    "job_201009151640_0013 3         1285107094387 s0894589 NORMAL   NA\n",
    "job_201009151640_0014 2         1285107283359 s0894589 NORMAL   NA\n",
    "job_201009151640_0015 2         1285109169514 s0894589 NORMAL   NA\n",
    "job_201009151640_0016 2         1285109271188 s0894589 NORMAL   NA\n",
    "job_201009151640_0018 1         1285148710573 s0894589 NORMAL   NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can leverage the grep command to narrow down the results to just see your own jobs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapred job -list all | grep $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Job Status\n",
    "\n",
    "To get the status of a particular job, we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapred job -status $jobid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Where the $jobid is ID of a job found in the first column of the list table above. The status will show the percent of completion of mappers and reducers, along with a tracking URL and the location of a file with all the information about the job. We will soon see that the web interface provides much more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Web Interface\n",
    "\n",
    "All of the previous actions can also be performed using the web interface. Open a browser and navigate to (or simply click if viewing in a browser) http://scutter02.inf.ed.ac.uk:8088. Note: you will need to be inside Informatics or use the VPN to see this page.\n",
    "\n",
    "This shows the web interface of the jobtracker. We can see a list of running, completed, and failed jobs. Clicking on a job ID is similar to requesting its status from the command line, but it shows much more details, including the number of bytes read/written to the filesystem, number of failed/killed task attempts, and nice graphs of job completion.\n",
    "Kill a Job\n",
    "\n",
    "To kill a job, run the following command, where $jobid is the ID of the job you want to kill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapred job -kill $jobid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "IMPORTANT NOTE: Make sure to kill any misbehaving jobs you have created using the above command ctr+C is NOT sufficient\n",
    "\n",
    "    TASK: Run through the following exercise\n",
    "\n",
    "To try this, copy the file /data/labs/source/sleeper.py from HDFS to somewhere on your local filesystem (not HDFS). Then run the following command by replacing the <path_for_sleeper.py> with the local path of sleeper.py (i.e./home/s1234567/sleeper.py). This command (which you will learn to interpret later in this lab) will create a Hadoop streaming Job that doesn’t actually do anything, except wait for you to kill it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /opt/hadoop/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar \\\n",
    " -input /data/labs/secondary.txt \\\n",
    " -output /user/$USER/data/sleep-output \\\n",
    " -mapper sleeper.py \\\n",
    " -file <path_for_sleeper.py> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open another terminal, log into the Hadoop cluster, and list all your jobs. Find the ID of the job you just started. Find out the status of your job, and finally kill it. After you run the kill command, look at the terminal where you originally started the job and watch the job die.\n",
    "\n",
    "NOTE: In any case that the above command fails, before re-executing it you need to execute the following (cleaning) command\n",
    "\n",
    "Execute the following to get rid of the output folder of the previous command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /user/$USER/data/sleep-output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hadoop examples are in /opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar which is a lot to type. So you might want to set an environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export EXAMPLES=/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar\n",
    "#Then we can use $EXAMPLES to refer to that path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing pi\n",
    "\n",
    "This example estimates the mathematical constant π\n",
    "to some error. The error depends on the number of samples we have (more samples → more accurate estimate). Run the example as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $EXAMPLES pi <num_maps> <num_samples>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where <num_maps> is the number of mapper jobs, and <num_samples> is the number of samples, for example using 10 mappers and 5 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $EXAMPLES pi 10 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following combinations for <num_maps> and <num_samples> and see how the running time and precision change:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Maps \tNumber of Samples \tTime (s) \tπ^\n",
    "2 \t10 \t\t\n",
    "5 \t10 \t\t\n",
    "10 \t10 \t\t\n",
    "2 \t100 \t\t\n",
    "10 \t100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Counting\n",
    "\n",
    "Hadoop has a number of demo applications and here we will look at the canonical task of word counting.\n",
    "\n",
    "    TASK: Try running through the following example\n",
    "\n",
    "We will count the number of times each word appears in a document. For this purpose, we will use the /data/labs/example3.txt file, so first copy that file to your input directory. Second, make sure you delete your output directory before running the job or the job will fail. We run the wordcount example by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $EXAMPLES wordcount /user/$USER/data/input /user/$USER/data/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where /user/$USER/data/input and /user/$USER/data/output are the input and output directories, respectively. After running the example, examine (using ls) the contents of the output directory. From the output directory, copy the file part-r-00000 to a local directory (somewhere in your home directory) and examine the contents. Was the job successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Streaming Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop streaming is a utility that allows you to create and run map/reduce jobs with any executable or script as the mapper and/or the reducer. The way it works is very simple: input is converted into lines which are fed to the stdin of the mapper process. The mapper processes this data and writes to stdout. You can learn more about stdin and stdout here.\n",
    "\n",
    "Lines from the stdout of the mapper process are converted into key/value pairs by splitting them on the first tab character (of course, this is only the default behavior and can be changed). The key/value pairs are fed to the stdin of the reducer process which collects and processes them.\n",
    "\n",
    "Finally, the reducer writes to stdout which is the final output of the program. Everything will become much clearer through examples later.\n",
    "\n",
    "It is important to note that with Hadoop streaming mappers and reducers can be any programs that read from stdin and write to stdout, so the choice of the programming language is left to the programmer. Here, we will use Python.\n",
    "\n",
    "\n",
    "#Writing a Word-Counting Program in Python 2.7.5\n",
    "\n",
    "In this subsection, we will see how to create a program in Python that can count the number of words of a specific file. Initially, we will test the code locally on small files before using it in a streaming MapReduce job.\n",
    "\n",
    "As we will see later, this is important as it helps in not running jobs in Hadoop that can give wrong results.\n",
    "#Word-Counting Python Mapper\n",
    "\n",
    "    Using Streaming, a Mapper reads from stdin and writes to stdout\n",
    "    Keys and Values are delimited (by default) using tabs\n",
    "    Records are split using newlines\n",
    "\n",
    "Create a file somewhere in your home directory called mapper.py -- there are a number of Python IDEs available, including PyCharm on DICE machines. Alternatively, simply use gedit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gedit mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the directory you want your mapper to be in. Then copy the following code into mapper.py and save. It's worth typing this out by hand rather than copy / pasting, to understand what the code is doing. If you are unfamiliar with the .format syntax of string interpolation Python, please refer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:                  # input from standard input\n",
    "    line = line.strip()                 # remove whitespaces\n",
    "    tokens = line.split()               # split the line into tokens\n",
    "\n",
    "    for token in tokens:                # write the results to standard output\n",
    "        print(\"{0}\\t{1}\".format(token, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you save the file mapper.py.\n",
    "\n",
    "IMPORTANT NOTE: for Hadoop to know how to properly run your Python scripts, you must include the following line as the first line in all your mappers and reducers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-Counting Python Reducer\n",
    "\n",
    "Create a file called reducer.py in the same directory as mapper.py, and copy the following code into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "prev_word = \"\"\n",
    "value_total = 0\n",
    "word = \"\"\n",
    "\n",
    "for line in sys.stdin:          # For ever line in the input from stdin\n",
    "    line = line.strip()         # Remove trailing characters\n",
    "    word, value = line.split(\"\\t\", 1)\n",
    "    value = int(value)\n",
    "    # Remember that Hadoop sorts map output by key reducer takes these keys sorted\n",
    "    if prev_word == word:\n",
    "        value_total += value\n",
    "    else:\n",
    "        if prev_word:  # write result to stdout\n",
    "            print(\"{0}\\t{1}\".format(prev_word, value_total))\n",
    "\n",
    "        value_total = value\n",
    "        prev_word = word\n",
    "\n",
    "if prev_word == word:  # Don't forget the last key/value pair\n",
    "    print(\"{0}\\t{1}\".format(prev_word, value_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, save this file before continuing.\n",
    "Testing the Code\n",
    "\n",
    "We perform local testing conforming to typical UNIX-style piping, our testing will take the form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat <data> | ./mapper.py | sort | ./reducer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which emulates the same pipeline that Hadoop will perform when streaming, albeit in a non-distributed manner. You have to make sure that files mapper.py and reducer.py have execution permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod u+x mapper.py\n",
    "chmod u+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following command and explain the results (hint: type man sort in your terminal window to find out more about the sort command):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "echo \"this is a test and this should count the number of words\" | ./mapper.py | sort -k1,1 | ./reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check\n",
    "\n",
    "The output from the above code should result in the following output:\n",
    "a       1\n",
    "and     1\n",
    "count   1\n",
    "is      1\n",
    "number  1\n",
    "of      1\n",
    "should  1\n",
    "test    1\n",
    "the     1\n",
    "this    2\n",
    "words   1\n",
    "\n",
    "TASK: Count the number of words a text file of your choosing contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a Streaming MapReduce Job\n",
    "\n",
    "The command syntax for creating a Hadoop Streaming MapReduce Job is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar <jar_path> [generic_options] [streaming_options] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming Options\n",
    "\n",
    "After running locally the code successfully, the next step is to run it in Hadoop. Suppose you have your mapper, mapper.py, and your reducer, reducer.py, while your input and output directories are <input> &<output> respectively.\n",
    "\n",
    "We always have to specify /opt/hadoop/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar as the jar to run, and the particular mapper and reducer we use are specified through -mapper and -reducer streaming options.\n",
    "\n",
    "In the case that the mapper and/or reducer are not already present on the remote machine (which will often be the case), we also have to package the actual files in the job submission. Assuming that neither mapper.py nor reducer.py were present on the machines in the cluster, the previous job would be run as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /opt/hadoop/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar \\\n",
    " -input <input> \\\n",
    " -output <output> \\\n",
    " -mapper mapper.py \\\n",
    " -file mapper.py \\\n",
    " -reducer reducer.py \\\n",
    " -file reducer.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the -file streaming option specifies that the file is to be copied to the cluster. This can be very useful for also packaging any auxiliary files your program might use (dictionaries, configuration files, etc). Each job can have multiple -file options.\n",
    "\n",
    "NOTE: In the latest version of Hadoop, the usage of multiple -file streaming options is deprecated. It is recommended to use a single -files generic option to specify all files separated by comma. The order of generic and streaming options is significant. The -files option must be placed before any streaming options or the command will fail (see next section for more details).\n",
    "\n",
    "    TASK: We will run a simple example to demonstrate how streaming jobs are run, follow these steps\n",
    "\n",
    "Copy the file: /data/labs/source/random-mapper.py from HDFS to a local directory (a directory on the local machine, not on HDFS.) This mapper simply generates a random number for each word in the input file, hence the input file in your input directory can be anything. Run the job by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /opt/hadoop/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar \\\n",
    " -input /user/$USER/data/input \\\n",
    " -output /user/$USER/data/output \\\n",
    " -mapper random-mapper.py \\\n",
    " -file random-mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    TASK: What happens when instead of using mapper.py you use /bin/cat as a mapper?\n",
    "\n",
    "    TASK: What happens when you use /bin/cat as both a mapper and reducer?\n",
    "\n",
    "Generic Options\n",
    "\n",
    "Various job generic options can be specified on the command line, we will cover the most used ones in this section. The general syntax for specifying additional configuration variables is -D <name>=<value>\n",
    "Setting Job Name\n",
    "\n",
    "To avoid having your job named something like streamjob5025479419610622742.jar, you can specify an alternative name through the mapreduce.job.name variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-D mapreduce.job.name=\"My job\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    TASK: Run the random-mapper.py example again, this time naming your job \"Random job <matriculation_number>\", where <matriculation_number> is your matriculation number.\n",
    "\n",
    "    After you run the job (and preferably before it finishes), open the browser and go to http://jobtracker.inf.ed.ac.uk:8088/cluster/nodes. In the list of running jobs look for the job with the name you gave it and click on it. You can see various statistics about your job -- try to find the number of reducers used. How many reducers did you use? If your job finished before you had a chance to open the browser, it will be in the list of finished jobs, not the list of running jobs, but you can still see all the same information by clicking on it.\n",
    "\n",
    "Key-Value Separation\n",
    "\n",
    "As was mentioned earlier, the key/value pairs are obtained by splitting the mapper output on the first tab character in the line. This can be changed using stream.map.output.field.separator and stream.num.map.output.key.fields variables. For example, if I want the key to be everything up to the second - character in the line, I would add the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-D stream.map.output.field.separator=- \\\n",
    "-D stream.num.map.output.key.fields=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop also comes with a partitioner class org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner which is useful for cases where you want to perform a secondary sort on the keys. Imagine you have the following list of IPs:\n",
    "\n",
    "192.168.2.1\n",
    "190.191.34.38\n",
    "161.53.72.111\n",
    "192.168.1.1\n",
    "161.53.72.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to partition the data so that addresses with the same parts up to the second dot are processed by the same reducer (first 16 bits of the address). However, you also want each reducer to see the data sorted according to the third dot (first 24 bits). Using the mentioned partitioner class you can tell Hadoop how to group the data to be processed by the reducers. You do this using the following options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-D mapreduce.map.output.key.field.separator=.\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first option tells Hadoop what character to use as a separator (just like in the previous example), and the second one tells how many fields from the key to use for partitioning. Knowing this, here is how we would solve the IP address example (assuming that the addresses are in /user/hadoop/input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /opt/hadoop/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar \\\n",
    " -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    " -D mapreduce.map.output.key.field.separator=. \\\n",
    " -D stream.map.output.field.separator=. \\\n",
    " -D stream.num.map.output.key.fields=3 \\\n",
    " -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "-input <input> \\\n",
    "-output <output> \\\n",
    "-mapper cat \\\n",
    "-reducer cat \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -D mapreduce.partition.keypartitioner.options=-k1,2 generic option tells Hadoop to partition IPs up to the second separator (the dot in this case), and -D stream.num.map.output.key.fields=3 tells it to sort the IPs according to everything before the third one -- which corresponds to the first 24 bits of the address. Finally the -partitioner <class_name> streaming option specifies the partitioner class that will be used.\n",
    "\n",
    "The option mapreduce.partition.keypartitioner.options lets you specify which fields to consider for partitioning by using comma separated values starting with -k. For example -k1,2 means keys one and two, -k1,1 means only key one and so forth. You can also specify which characters of each key should be considered by using dots. For example -k1.2,1.5 means partition by key one character two until key one character five. You can find a more detailed specification in the java docs of setKeyFieldPartitionerOptions. This option is also used for secondary sorting as you will learn below.\n",
    "\n",
    "IMPORTANT NOTE: The order of the arguments is significant. Generics arguments such as the ones starting with -D should be placed before the streaming options, otherwise the command will fail.\n",
    "\n",
    "IMPORTANT NOTE: The option num.key.fields.for.partition is deprecated. Technically it tells Hadoop how many fields to consider for partitioning but it does not work well together with the more powerful mapreduce.partition.keypartitioner.options.\n",
    "Partitioning & Secondary Sorting\n",
    "\n",
    "The secondary.txt file (/data/labs/secondary.txt) contains lines in the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_name.first_name.address.phone_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to partition the data so that all the people with the same first and last name go to the same reducer, and that they are sorted according to address. An implementation is given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /opt/hadoop/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar \\\n",
    " -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    " -D mapreduce.map.output.key.field.separator=. \\\n",
    " -D stream.map.output.field.separator=. \\\n",
    " -D stream.num.map.output.key.fields=3 \\\n",
    " -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    " -D mapreduce.partition.keycomparator.options=-k3 \\\n",
    "-input /data/labs/secondary.txt \\\n",
    "-output /user/$USER/data/output \\\n",
    "-mapper cat \\\n",
    "-reducer cat \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdfs dfs -cat /user/$USER/data/output/* returns the expected output as follows:\n",
    "\n",
    "Stanley.Cup.Elm street 1        555-1002\n",
    "Thayer.Tommy.Elm street 4       555-1003\n",
    "Singer.Eric.Elm street 2        555-1001\n",
    "Stanley.Paul.Elm street 3       555-1002\n",
    "Simmons.Gene.Elm street 1       555-1000\n",
    "Simmons.Gene.Elm street 5       555-666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key points are the configurations of the KeyFieldBasedComparator class and the KeyFieldBasedPartitioner class. The documentation provides further examples of how you may use these features.\n",
    "\n",
    "Note that multiple options mean that they should be enclosed in quotation marks, unlike the previous example of just -k3. If we wanted to sort by the first column in descending order, and both the second column, and then the third column, in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k1,1r -k2,2 -k3,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields the result:\n",
    "\n",
    "Thayer.Tommy.Elm street 4       555-1003\n",
    "Stanley.Cup.Elm street 1        555-1002\n",
    "Singer.Eric.Elm street 2        555-1001\n",
    "Stanley.Paul.Elm street 3       555-1002\n",
    "Simmons.Gene.Elm street 1       555-1000\n",
    "Simmons.Gene.Elm street 5       555-666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your keys are numeric, you need to us the -n modified. For instance, imagine the first key is a name, and the second is an age (integer), to sort both in descending order (note the -n and -r become nr after the index notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-D mapreduce.partition.keycomparator.options=\"-k1,1r -k2,2nr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    TASK: Using what you have learned in this section and the secondary as your input file, your task is to:\n",
    "\n",
    "    Partition the data so that all people with the same last name go to the same reducer.\n",
    "    Partition the data so that all people with the same last name go to the same reducer, and also make sure that the lines are sorted according to first name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Running a MapReduce Program in Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    If you are unfamiliar with Java, or would like to know more, it is recommended that you review relevant video lectures and materials from the IJP course.\n",
    "\n",
    "Setting Up the Environment\n",
    "\n",
    "You can either set these environment variables per-session, or append them to your .bash_profile file in your home directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-sun-1.8.0.144/\n",
    "export PATH=${JAVA_HOME}/bin:${PATH}\n",
    "export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Code (Adapted from Hadoop Documentation)\n",
    "\n",
    "Create the java class file WordCount.java with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "  public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n",
    "    // Making objects is expensive. Instantiate outside the loop and re-use\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "\n",
    "      // Whilst iterating over the token iterator\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        word.set(itr.nextToken());  // Store the next token in our Text object\n",
    "        context.write(word, one);  // Give a <word, 1> pair\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "      int sum = 0;\n",
    "\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "\n",
    "      result.set(sum);\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "\n",
    "    // Make this class the main in the JAR file\n",
    "    job.setJarByClass(WordCount.class);\n",
    "\n",
    "    // Set out Mapper class, conforming to the API\n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "\n",
    "    // Set out Combiner & Reducer classes, conforming to the (same) API\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "\n",
    "    // Set the ouput Key type\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "\n",
    "    // Set the output Value type\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    // Set number of reducers\n",
    "    job.setNumReduceTasks(10);\n",
    "\n",
    "    // Get the input and output paths from the job arguments\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the Java Code\n",
    "\n",
    "We're going to compile this Java code into a JAR file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop com.sun.tools.javac.Main WordCount.java\n",
    "jar cf mywordcount.jar WordCount*.class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a note, the number of mappers can be suggested via a command line argument -D mapred.map.tasks=5, but ultimately the InputFormat will decide upon the number needed.\n",
    "Running the Job\n",
    "\n",
    "Now, deploy the JAR file with the input data noted below, and the output directory we just created in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar mywordcount.jar WordCount /data/labs/example3.txt /user/$USER/data/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check - Output\n",
    "\n",
    "Print the top 10 lines of the output to the terminal and compare it to the output below. If all was done correctly, they should be the same. If not, check over your code and try again. If that still doesn't work, ask for help!\n",
    "\n",
    "To print the top 10 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/$USER/data/output/* | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your output matches the following:\n",
    "\n",
    "But     1\n",
    "ask     1\n",
    "both    1\n",
    "desert  2\n",
    "up      6\n",
    "you     21\n",
    "aching  1\n",
    "been    2\n",
    "down    2\n",
    "game    1\n",
    "\n",
    "    TASK: Run again the same task as previously but use 5 reducers instead of 10. Observe the output folder and the number of files produced.\n",
    "\n",
    "    TASK: Run the example of the wordcount problem from the first lab by using only two reducers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Side Information\n",
    "\n",
    "It is often useful to package other, external files together with the job. For example, if your application uses a dictionary or a file that stores some configuration settings, one would want these files to be available to the program just as they would in a non-mapreduce setting. This can be achieved using the -file option that we already used to package the source files. The following program takes a dictionary and counts only those words that appear in the dictionary, ignoring everything else. First copy the /data/labs/source/mapper-dict.py and /data/labs/source/reducer.py to a local directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -get /data/labs/source/mapper-dict.py ~/\n",
    "hdfs dfs -get /data/labs/source/reducer.py ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy the dictionary to a local directory(-get and -copyToLocal provide the same functionality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs dfs -copyToLocal /data/labs/dict.eng ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: Run the program by typing (assuming you still have example3.txt in your input directory and that your output directory doesn’t exist):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /opt/hadoop/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar \\\n",
    " -input  /user/$USER/data/example3.txt \\\n",
    " -output /user/$USER/data/output \\\n",
    " -mapper mapper-dict.py \\\n",
    " -file mapper-dict.py \\\n",
    " -reducer reducer.py \\\n",
    " -file reducer.py \\\n",
    " -file dict.eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program will use dict.eng as the dictionary and count only those words that appear in that list. Look at the source of mapper-dict.py to see how to open the dictionary file.\n",
    "Further Resources\n",
    "\n",
    "You now have a broad knowledge of the way in which hadoop is used. If you'd like to know more, please use the following resources:\n",
    "\n",
    "    Official Hadoop Documention\n",
    "    Hadoop Cheat Sheet\n",
    "    Hadoop for Dummies Cheat Sheet\n",
    "    YouTube Playlist -- Hadoop Tutorials\n",
    "\n",
    "These are presented as optional reading, and good places to consult if you're stuck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
