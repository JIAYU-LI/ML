{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readin_large_data():\n",
    "    \"\"\"Read small data set into a 2D numpy array\n",
    "    return: numpy array contains the small data set, shape: (50,000, 230)\n",
    "    \"\"\"\n",
    "    PATH = os.getcwd()\n",
    "    \n",
    "    DATA_PATH = os.path.join(PATH, 'data_large','orange_large_train.data','orange_large_train.data.chunk' + str(i))\n",
    "    \n",
    "    if i == 1:\n",
    "        data = pd.read_csv(DATA_PATH, delimiter = '\\t')\n",
    "    else:\n",
    "        data = pd.read_csv(DATA_PATH, delimiter='\\t', header=None)\n",
    "    data_type = {key:np.float64 for key in header[:190]}\n",
    "    data_type.update({key:str for key in header[190:]})\n",
    "    return data\n",
    "\n",
    "large_data = readin_large_data(1)\n",
    "print(large_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1719186/miniconda3/envs/dme/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (14741,14742,14748,14749,14750,14752,14753,14755,14758,14759,14760,14762,14767,14769,14773,14776,14779,14780,14781,14789,14791,14797,14798,14800,14804,14805,14806,14809,14811,14817,14818,14819,14824,14828,14832,14834,14836,14843,14844,14850,14851,14855,14861,14871,14876,14878,14879,14881,14882,14883,14886,14888,14890,14894,14896,14897,14900,14904,14906,14914,14916,14917,14918,14926,14928,14931,14932,14936,14937,14939,14942,14944,14947,14951,14954,14957,14958,14970,14972,14980,14996,14998) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/afs/inf.ed.ac.uk/user/s17/s1719186/miniconda3/envs/dme/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (14741,14742,14748,14749,14750,14752,14753,14755,14758,14759,14760,14762,14767,14769,14773,14776,14779,14780,14781,14789,14791,14797,14798,14800,14804,14805,14806,14809,14811,14817,14818,14819,14824,14828,14832,14834,14836,14843,14844,14850,14851,14855,14861,14876,14878,14879,14881,14882,14883,14886,14888,14890,14894,14896,14897,14900,14904,14906,14914,14916,14917,14918,14926,14928,14931,14932,14936,14937,14939,14942,14944,14947,14951,14954,14957,14958,14970,14972,14980,14996,14998) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pecent of missing value:  0.033478092\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "DATA = os.path.join(PATH, 'data_large', 'orange_large_train.data','orange_large_train.data.chunk1')\n",
    "with open(DATA) as f:\n",
    "\theader = f.readline().strip('\\n').split('\\t')\n",
    "data_type = {key:np.float64 for key in header[:14740]}\n",
    "data_type.update({key:str for key in header[14740:]})\n",
    "large_data = pd.read_table(DATA, dtype=data_type)\n",
    "\n",
    "for i in range(2, 6):\n",
    "\tDATA = os.path.join(PATH,'data_large', 'orange_large_train.data','orange_large_train.data.chunk' + str(i))\n",
    "\ttemp = pd.read_table(DATA, header=None, dtype=data_type)\n",
    "\ttemp.columns=header\n",
    "\tlarge_data = large_data.append(temp)\n",
    "\n",
    "print('Pecent of missing value: ', large_data.isnull().sum().sum() / (large_data.shape[0] * large_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_rate = large_data.isnull().sum() / large_data.shape[0]\n",
    "\n",
    "numerical_data = large_data.loc[:, large_data.columns[:-260]]\n",
    "numerical_missing_rate = numerical_data.isnull().sum() / numerical_data.shape[0]\n",
    "useful_numerical_data = numerical_data.loc[:, numerical_missing_rate < 0.5]\n",
    "\n",
    "categorical_data = large_data.loc[:, large_data.columns[-260:]]\n",
    "categorical_missing_data = categorical_data.isnull().sum() / categorical_data.shape[0]\n",
    "useful_categorical_data = categorical_data.loc[:, categorical_missing_data < 0.5]\n",
    "\n",
    "# useful_numerical_data.to_csv('numerical_data.csv')\n",
    "# useful_categorical_data.to_csv('categorical_data.csv')\n",
    "# sns.distplot(numerical_data.isnull().sum(axis=1) / data.shape[0], kde=False, rug=True, bins=10)\n",
    "# t=numerical_data.isnull().sum(axis=1) / numerical_data.shape[1]\n",
    "clean_numerical_data = useful_numerical_data.fillna(useful_numerical_data.mean())\n",
    "clean_categorical_data = useful_categorical_data.fillna(useful_categorical_data.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 14438)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_numerical_data.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 36)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_categorical_data.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_numerical_data.to_csv('large_clean_numerical_data.csv')\n",
    "clean_categorical_data.to_csv('large_categorical_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read labels and run experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readin_label(target):\n",
    "    \"\"\"Read upselling, churn or appetency label into a 2D numpy array\n",
    "    parameter target: choose which label to load\n",
    "    return: numpy array contains the specified label of small data set, shape: (50,000, 1)\n",
    "    \"\"\"\n",
    "    assert target in ['upselling', 'churn', 'appetency']\n",
    "    PATH = os.getcwd()\n",
    "    LABEL_PATH = os.path.join(PATH, 'data_large','orange_large_train_'+target+'.labels')\n",
    "\n",
    "    label = pd.read_csv(LABEL_PATH, delimiter='\\t',prefix=target+'_label')\n",
    "    \n",
    "#     label = label[:10000,:]\n",
    "    \n",
    "    label[label==1] = True\n",
    "    label[label==-1] = False\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Percent of upselling: ', np.sum(upselling_label) / len(upselling_label))\n",
    "print('Percent of churn: ', np.sum(churn_label) / len(churn_label))\n",
    "print('Percent of appetency: ', np.sum(appetency_label) / len(appetency_label))\n",
    "\n",
    "print('Pecent of missing value: ', np.sum(data == None) / (data.shape[0] * data.shape[1]))\n",
    "print('Pecent of missing value: ', data.isnull().sum().sum() / (data.shape[0] * data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upselling_label = readin_label('upselling')\n",
    "print(upselling_label.shape)\n",
    "churn_label = readin_label('churn')\n",
    "print(churn_label.shape)\n",
    "appetency_label = readin_label('appetency')\n",
    "print(appetency_label.shape)\n",
    "labels = np.hstack([appetency_label, churn_label, upselling_label])\n",
    "labels = np.hstack([labels, ~np.any(labels, axis=1)[:,np.newaxis]])\n",
    "labels = labels.astype(int)\n",
    "\n",
    "labels_current = labels[:9999,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=100, activation='relu', input_dim=14438))\n",
    "keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "# model.add(Dense(units=100, activation='relu'))\n",
    "# keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "# model.add(Dense(units=100, activation='relu'))\n",
    "# keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# trainX, valX, trainY, valY = train_test_split(clean_numerical_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(sklearn.preprocessing.robust_scale(clean_numerical_data), labels_current, epochs=100, batch_size=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
