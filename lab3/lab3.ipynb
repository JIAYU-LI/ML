{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNLP: Lab Session 3\n",
    "\n",
    "### Hidden Markov Models - Construction and Use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages used for this lab\n",
    "\n",
    "import nltk\n",
    "\n",
    "#import brown corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# module for training a Hidden Markov Model and tagging sequences\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "# module for computing a Conditional Frequency Distribution\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# module for computing a Conditional Probability Distribution\n",
    "from nltk.probability import ConditionalProbDist\n",
    "\n",
    "# module for computing a probability distribution with the Maximum Likelihood Estimate\n",
    "from nltk.probability import MLEProbDist\n",
    "\n",
    "import operator\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Corpora tagged with part-of-speech information\n",
    "\n",
    "NLTK provides corpora annotated with part-of-speech (POS) information and\n",
    "some tools to access this information. The Penn Treebank tagset is commonly\n",
    "used for annotating English sentences. We can inspect this tagset in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus provided with NLTK is also tagged with POS information,\n",
    "although the tagset is slightly different than the Penn Treebank tagset. Information about the Brown corpus tagset can be found here:\n",
    "http://www.scs.leeds.ac.uk/ccalas/tagsets/brown.html\n",
    "\n",
    "We can retrieve the tagged sentences in the Brown corpus by calling the `tagged_sents()`\n",
    "function and looking at an annotated sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = brown.tagged_sents(categories= 'news')\n",
    "print tagged_sentences[29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to use a coarser label set in order to avoid data sparsity\n",
    "or to allow a mapping between the POS labels for different languages. The Universal tagset was designed to be applicable for all languages:\n",
    "\n",
    "https://code.google.com/p/universalpostags/.\n",
    "\n",
    "There are mappings between the POS tagset of several languages and the Universal tagset. We can access the Universal tags for the Brown corpus sentences\n",
    "by changing the tagset argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences_universal = brown.tagged_sents(categories= 'news', tagset='universal')\n",
    "print tagged_sentences_universal[29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1:\n",
    "\n",
    "In this exercise we will compute a Frequency Distribution over tags that appear\n",
    "in the Brown corpus. The template of the function that you have to implement\n",
    "takes two parameters: one is the category of the text and the other is the tagset\n",
    "name. You are given the code to retrieve the list of (word, tag) tuples from the\n",
    "brown corpus corresponding to the given category and tagset.\n",
    "\n",
    "1. Convert the list of word+tag pairs to a list of tags\n",
    "2. Using the list of tags to compute a frequency distribution over the tags, useing `FreqDist()`\n",
    "3. Compute the total number of tags in the Frequency Distribution\n",
    "4. Retrieve the top 10 most frequent tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# EXERCISE 1 #################\n",
    "# Solution for exercise 1\n",
    "# Input: genre (string), tagset (string)\n",
    "# Output: number_of_tags (int), top_tags (list of string)\n",
    "\n",
    "\n",
    "# get the number of tags found in the corpus\n",
    "# compute the Frequency Distribution of tags\n",
    "\n",
    "def ex1(genre,tagset):\n",
    "  \n",
    "    # get the tagged words from the corpus\n",
    "    tagged_words = brown.tagged_words(categories= genre, tagset=tagset)\n",
    "  \n",
    "    # TODO: build a list of tags\n",
    "    tags = \n",
    "  \n",
    "    # TODO: using the above list compute the Frequency Distribution of tags in the corpus\n",
    "    # hint: use nltk.FreqDist()\n",
    "    tagsFDist = \n",
    "  \n",
    "    # TODO: retrieve the total number of tags in the tagset\n",
    "    number_of_tags = \n",
    "  \n",
    "    #TODO: retrieve the top 10 most frequent tags\n",
    "    top_tags = \n",
    "\n",
    "    return (number_of_tags,top_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code for excercise 1\n",
    "\n",
    "def test_ex1():\n",
    "    print \"Tag FreqDist for news:\"\n",
    "    print ex1('news',None)\n",
    "  \n",
    "    print \"Tag FreqDist for science_fiction:\"\n",
    "    print ex1('science_fiction',None)\n",
    "  \n",
    "    # Do the same thing for a different tagset: Universal. Observe differences\n",
    "  \n",
    "    print \"Tag FreqDist for news with Universal tagset:\"\n",
    "    print ex1('news','universal')\n",
    "  \n",
    "    print \"Tag FreqDist for science_fiction with Universal tagset:\"\n",
    "    print ex1('science_fiction','universal')\n",
    "\n",
    "\n",
    "# Let's look at the top tags for different genre and tagsets. Observe differences\n",
    "test_ex1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating an HMM Tagger\n",
    "\n",
    "NLTK provides a module for training a Hidden Markov Model for sequence tag-\n",
    "ging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.tag.hmm.HiddenMarkovModelTagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the HMM for POS tagging given a labelled dataset. In Section 1\n",
    "of this lab we learned how to access the labelled sentences of the Brown corpus.\n",
    "We will use this dataset to study the effect of the size of the training corpus on\n",
    "the accuracy of the tagger.\n",
    "\n",
    "#### Exercise 2:\n",
    "\n",
    "In this exercise we will train a HMM tagger on a training set and evaluate it\n",
    "on a test set. The template of the function that you have to implement takes\n",
    "two parameters: a sentence to be tagged and the size of the training corpus in\n",
    "number of sentences. You are given the code that creates the training and test\n",
    "datasets from the tagged sentences in the Brown corpus.\n",
    "\n",
    "1. Train a Hidden Markov Model tagger on the training dataset. Refer to `help(nltk.tag.hmm.HiddenMarkovModelTagger.train)` if necessary.\n",
    "2. Use the trained model to tag the sentence\n",
    "3. Use the trained model to evaluate the tagger on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# EXERCISE 2 #################\n",
    "# Solution for exercise 2\n",
    "# Input: sentence (list of string), size (<4600)\n",
    "# Output: hmm_tagged_sentence (list of tuples), tagger (HiddenMarkovModelTagger)\n",
    "\n",
    "# hint: use the help on HiddenMarkovModelTagger to find out how to train, tag and evaluate using this module\n",
    "def ex2(sentence, size):\n",
    "  \n",
    "    tagged_sentences = brown.tagged_sents(categories= 'news')\n",
    "    #ASSERT (0<size<1)\n",
    "  \n",
    "    # set up the training data\n",
    "    train_data = tagged_sentences[-size:]\n",
    "  \n",
    "    # set up the test data\n",
    "    test_data = tagged_sentences[:100]\n",
    "\n",
    "    # TODO: train a HiddenMarkovModelTagger\n",
    "    # hint: use the train() method\n",
    "    tagger = \n",
    "\n",
    "    # TODO: using the hmm tagger tag the sentence\n",
    "    hmm_tagged_sentence = \n",
    "  \n",
    "    # TODO: using the hmm tagger evaluate on the test data\n",
    "    eres = \n",
    "\n",
    "    return (tagger, hmm_tagged_sentence,eres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code for excercise 2\n",
    "def test_ex2():\n",
    "    tagged_sentences = brown.tagged_sents(categories= 'news')\n",
    "    words = [tp[0] for tp in tagged_sentences[42]]\n",
    "    (tagger, hmm_tagged_sentence, eres ) = ex2(words,500)\n",
    "    print \"Sentenced tagged with nltk.HiddenMarkovModelTagger:\"\n",
    "    print hmm_tagged_sentence\n",
    "    print \"Eval score:\"\n",
    "    print eres\n",
    "  \n",
    "    (tagger, hmm_tagged_sentence, eres ) = ex2(words,3000)\n",
    "    print \"Sentenced tagged with nltk.HiddenMarkovModelTagger:\"\n",
    "    print hmm_tagged_sentence\n",
    "    print \"Eval score:\"\n",
    "    print eres\n",
    "\n",
    "\n",
    "#Look at the tagged sentence and the accuracy of the tagger. How does the size of the training set affect the accuracy?\n",
    "test_ex2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Transition and Emission Probabilities\n",
    "\n",
    "In the previous exercise we learned how to train and evaluate an HMM tagger.\n",
    "We have used the HMM tagger as a black box and have seen how the training\n",
    "data affects the accuracy of the tagger. In order to get a better understanding\n",
    "of the HMM we will look at the two components of this model:\n",
    "    \n",
    "* The transition model\n",
    "* The emission model\n",
    "\n",
    "The transition model estimates $P (tag_{i+1} |tag_i )$, the probability of a POS tag\n",
    "at position $i+1$ given the previous tag (at position $i$). The emission model\n",
    "estimates $P (word|tag)$, the probability of the observed word given a tag.\n",
    "\n",
    "Given the above definitions, we will need to learn a Conditional Probability\n",
    "Distribution for each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.probability.ConditionalProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "In this exercise we will estimate the emission model. In order to compute the\n",
    "Conditional Probability Distribution of $P (word|tag)$ we first have to compute\n",
    "the Conditional Frequency Distribution of a word given a tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.probability.ConditionalFreqDist)\n",
    "help(nltk.probability.ConditionalProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor of the ConditionalFreqDist class takes as input a list of tuples,\n",
    "each tuple consisting of a condition and an observation. For the emission model,\n",
    "the conditions are tags and the observations are the words. The template of the\n",
    "function that you have to implement takes as argument the list of tagged words\n",
    "from the Brown corpus.\n",
    "\n",
    "1. Build the dataset to be passed to the `ConditionalFreqDist()` constructor. Words should be lowercased. Each item of data should be a tuple of tag (a condition) and word (an observation).\n",
    "2. Compute the Conditional Frequency Distribution of words given tags.\n",
    "3. Return the top 10 most frequent words given the tag NN.\n",
    "4. Compute the Conditional Probability Distribution for the above Conditional Frequency Distribution. Use the `MLEProbDist` estimator when calling the ConditionalProbDist constructor.\n",
    "5. Compute the probabilities:\n",
    "\n",
    " $ P(year|N N ) $ \n",
    " \n",
    " $P(year|DT)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############# EXERCISE 3 #################\n",
    "# Solution for exercise 3\n",
    "# Input: tagged_words (list of tuples)\n",
    "# Output: emission_FD (ConditionalFreqDist), emission_PD (ConditionalProbDist), p_NN (float), p_DT (float)\n",
    "\n",
    "\n",
    "# in the previous labs we've seen how to build a freq dist\n",
    "# we need conditional distributions to estimate the transition and emission models\n",
    "# in this exerise we estimate the emission model\n",
    "def ex3(tagged_words):\n",
    "\n",
    "    # TODO: prepare the data\n",
    "    # the data object should be a list of tuples of conditions and observations\n",
    "    # in our case the tuples will be of the form (tag,word) where words are lowercased\n",
    "    data = \n",
    "\n",
    "    # TODO: compute a Conditional Frequency Distribution for words given their tags using our data\n",
    "    emission_FD = \n",
    "\n",
    "    # TODO: return the top 10 most frequent words given the tag NN\n",
    "    top_NN = \n",
    "\n",
    "    # TODO: Compute the Conditional Probability Distribution using the above Conditional Frequency Distribution. Use MLEProbDist estimator.\n",
    "    emission_PD = \n",
    "\n",
    "    # TODO: compute the probabilities of P(year|NN) and P(year|DT)\n",
    "    p_NN = \n",
    "    p_DT = \n",
    "\n",
    "    return (emission_FD, top_NN, emission_PD, p_NN, p_DT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your solution for excersise 3\n",
    "\n",
    "def test_ex3():\n",
    "    tagged_words = brown.tagged_words(categories='news')\n",
    "    (emission_FD, top_NN, emission_PD, p_NN, p_DT) = ex3(tagged_words)\n",
    "    print \"Frequency of words given the tag *NN*: \", top_NN\n",
    "    print \"P(year|NN) = \", p_NN\n",
    "    print \"P(year|DT) = \", p_DT\n",
    "\n",
    "\n",
    "#Look at the estimated probabilities. Why is P(year|DT) = 0 ? What are the problems with having 0 probabilities and what can be done to avoid this?\n",
    "test_ex3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the problems with having zero probabilities and what can be done to\n",
    "avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "In this exercise we will estimate the transition model. In order to compute the\n",
    "Conditional Probability Distribution of $P (tag_{i+1} |tag_i )$ we first have to compute\n",
    "the Conditional Frequency Distribution of a tag at position $i + 1$ given the previous tag.\n",
    "\n",
    "The constructor of the `ConditionalFreqDist` class takes as input a list of tuples, each tuple consisting of a condition and an observation. For the transition\n",
    "model, the conditions are tags at position i and the observations are tags at\n",
    "position $i + 1$. The template of the function that you have to implement takes\n",
    "as argument the list of tagged sentences from the Brown corpus.\n",
    "\n",
    "1. Build the dataset to be passed to the `ConditionalFreqDist()` constructor. Each item in your data should be a pair of condition and observation: $(tag_i,tag_{i+1})$\n",
    "2. Compute the Conditional Frequency Distribution of a tag at position $i + 1$ given the previous tag.\n",
    "3. Compute the Conditional Probability Distribution for the above Conditional Frequency Distribution. Use the `MLEProbDist` estimator when calling the `ConditionalProbDist` constructor.\n",
    "4. Compute the probabilities \n",
    "   \n",
    "   $P(N N|V BD)$ \n",
    "   \n",
    "   $P(N N|DT)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# EXERCISE 4 #################\n",
    "# Solution for exercise 4\n",
    "# Input: tagged_sentences (list)\n",
    "# Output: emission_FD (ConditionalFreqDist), emission_PD (ConditionalProbDist), p_VBD_NN, p_DT_NN\n",
    "\n",
    "# compute the transition probabilities\n",
    "# the probabilties of a tag at position i+1 given the tag at position i\n",
    "def ex4(tagged_sentences):\n",
    "\n",
    "    # TODO: prepare the data\n",
    "    # the data object should be an array of tuples of conditions and observations\n",
    "    # in our case the tuples will be of the form (tag_(i),tag_(i+1))\n",
    "    data = \n",
    "\n",
    "    # TODO: compute the Conditional Frequency Distribution for a tag given the previous tag\n",
    "    # hint: use the ConditionalFreqDist()\n",
    "    transition_FD =\n",
    "\n",
    "    # TODO: compute the transition probability P(tag_(i+1)|tag_(i)) using the MLEProbDist to estimate the probabilities\n",
    "    # hint: use ConditionalProbDist()\n",
    "    transition_PD =\n",
    "\n",
    "    # TODO: compute the probabilities of P(NN|VBD) and P(NN|DT)\n",
    "    p_VBD_NN = \n",
    "    p_DT_NN = \n",
    "\n",
    "    return (transition_FD, transition_PD,p_VBD_NN, p_DT_NN )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your solution for excercise 4\n",
    "\n",
    "def test_ex4():\n",
    "  tagged_sentences = brown.tagged_sents(categories= 'news')\n",
    "  (transition_FD, transition_PD,p_VBD_NN, p_DT_NN ) = ex4(tagged_sentences)\n",
    "  print \"P(NN|VBD) = \", p_VBD_NN\n",
    "  print \"P(NN|DT) = \", p_DT_NN\n",
    "\n",
    "    \n",
    "# Are the results what you would expect? The sequence NN DT seems very probable. How will this affect the sequence tagging?\n",
    "test_ex4()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "Modify your code for exercise 3 to use a different estimator, to introduce some\n",
    "smoothing, and compare the results with the original.\n",
    "In exercise 4 we didn’t do anything about the boundaries. Modify your code for\n",
    "exercise 4 to use < s> at the beginning of every sentence and </ s> at the end.\n",
    "\n",
    "Explore the resulting conditional probabilities. What is the most likely tag at\n",
    "the beginning of a sentence? At the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
